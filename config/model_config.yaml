# NextGenTorch Model Configuration

# GPT model configurations
gpt:
  2B:
    layers: 24
    hidden_size: 1024
    ff_size: 4096
    attention_heads: 16
    vocab_size: 50000
  7B:
    layers: 32
    hidden_size: 2048
    ff_size: 8192
    attention_heads: 32
    vocab_size: 50000

# BERT model configurations
bert:
  base:
    layers: 12
    hidden_size: 768
    ff_size: 3072
    attention_heads: 12
    vocab_size: 30522
  large:
    layers: 24
    hidden_size: 1024
    ff_size: 4096
    attention_heads: 16
    vocab_size: 30522

# RoBERTa model configurations
roberta:
  base:
    layers: 12
    hidden_size: 768
    ff_size: 3072
    attention_heads: 12
    vocab_size: 50265
  large:
    layers: 24
    hidden_size: 1024
    ff_size: 4096
    attention_heads: 16
    vocab_size: 50265

# T5 model configurations
t5:
  small:
    layers: 6
    hidden_size: 512
    ff_size: 2048
    attention_heads: 8
    vocab_size: 32128
  base:
    layers: 12
    hidden_size: 768
    ff_size: 3072
    attention_heads: 12
    vocab_size: 32128

# ALBERT model configurations
albert:
  base:
    layers: 12
    hidden_size: 768
    ff_size: 3072
    attention_heads: 12
    vocab_size: 30000
  large:
    layers: 24
    hidden_size: 1024
    ff_size: 4096
    attention_heads: 16
    vocab_size: 30000

# Common model settings
common:
  dropout_rate: 0.1
  activation_function: "gelu"
  layer_norm_epsilon: 1e-5

# Tokenizer settings
tokenizer:
  type: "byte-level BPE"
  max_length: 2048
